{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Build_model_torch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMbFNE7kRHBQyhYqmPMNK4g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidho27941/ML_tutorial_notebook/blob/main/Build_model_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 基於PyTorch平台的基本模型建立\n",
        "\n",
        "## 載入必要函式庫以及資料集\n",
        "\n",
        "為了在PyTorch建立模型，我們需要導入以下函式庫：\n",
        "\n",
        "* torch \n",
        "* torchvision\n",
        "\n",
        "以及常用的資料處理函式庫：\n",
        "\n",
        "* Numpy \n",
        "* Pandas\n",
        "\n",
        "我們可以透過`torch.cuda.is_available()`來確認GPU是否可用，並選擇在CPU或是GPU上運行我們的程式。"
      ],
      "metadata": {
        "id": "JkRtRK9j_Osc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l05jzHHeE7RE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15c3a900-f263-4dff-8785-162f1a9071c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device State: cpu\n"
          ]
        }
      ],
      "source": [
        "import torchvision \n",
        "import torch \n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "# GPU\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device State:', device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 匯入MNIST資料集\n",
        "\n",
        "在此範例中，我們一樣會使用MNIST資料集進行示範。\n",
        "在載入時，我們需要使用`torchvision`的`datasets.MNIST`函式來抽取MNSIT資料集。並利用經由`torchvision.transforms.Compose`所整理的轉置流程對資料集進行轉置。\n",
        "\n",
        "我們將分別導入訓練資料集以及測試資料集作為訓練以及測試使用。"
      ],
      "metadata": {
        "id": "tT4oZPQcLrCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform\n",
        "# Step 1: Convert to Tensor\n",
        "# Step 2: Normalize with mean = 0.5 and std= 0.5\n",
        "transform = torchvision.transforms.Compose(\n",
        "    [torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize((0.5,), (0.5,)),]\n",
        ")\n",
        "\n",
        "# Obtain MNIST training dataset.\n",
        "# Transform is a custom functon define by user, which allow user to apply the transformation when loading dataset.\n",
        "mnist_train = torchvision.datasets.MNIST(root='MNIST', \n",
        "                            download=True, \n",
        "                            train=True, \n",
        "                            transform=transform)\n",
        "                            \n",
        "# Obtain MNIST test dataset\n",
        "mnist_test = torchvision.datasets.MNIST(root='MNIST', \n",
        "                            download=True, \n",
        "                            train=False, \n",
        "                            transform=transform)"
      ],
      "metadata": {
        "id": "ELM5IzM0JD5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 將資料打包並載入\n",
        "\n",
        "在我們利用`torchvision.datasets.MNIST`函式將資料抽出後，我們可以利用`torch.utils.data.DataLoader`將資料轉置並載入成一個物件。\n",
        "\n",
        "在`DataLoader`函式中，可以宣告：\n",
        "\n",
        "* batch_size: 宣告批次數量大小。\n",
        "* shuffle: 宣告是否要重新排序。\n",
        "* sampler: 宣告自定義採樣器。\n",
        "\n",
        "> 詳細參數可至[官方文件](https://pytorch.org/docs/stable/data.html)做進一步了解。"
      ],
      "metadata": {
        "id": "5lW9FPodY9A7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainLoader = torch.utils.data.DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
        "testLoader = torch.utils.data.DataLoader(mnist_test, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "vCmorvLTJfjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 建立順序式模型\n",
        "\n",
        "在PyTorch之中，建立簡單的順序式模型與`TensorFlow`的作法相當類似。在`TensorFlow`中，我們利用`tf.keras.Sequential`函式來建立順序式模型，而在`PyTorch`之中我們使用`torch.nn`下屬的`torch.nn.Sequential`函數來完成順序式模型的建立。\n",
        "\n",
        "以下是利用`torch.nn.Sequential`建立一個多層感知器（MLP）的範例。我們在一個順序性模型之中建立了三層的全連結層（利用`nn.Linear`）並結合線性整流函數(ReLU)以及對數歸一化指數函數（LogSoftmax）作為激勵函數。在模型建立完成之後，我們利用`.to()`將模型送到指定的裝置上進行使用。\n"
      ],
      "metadata": {
        "id": "pz_taBZSZT0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=784, out_features=128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(in_features=128, out_features=64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(in_features=64, out_features=10),\n",
        "            torch.nn.LogSoftmax(dim=1)\n",
        "            ).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p44ARojPJq0K",
        "outputId": "4c19d8ed-3dc3-4545-cd7d-d21293746b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (5): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 設定各項超參數、損失函數以及優化器\n",
        "\n",
        "EPOCHS = 10 #@param {type:\"slider\", min:1, max:100, setp: 1}\n",
        "LR = 1e-2 #@param {type:\"number\"}\n",
        "OPTIMIZER = 'adam' #@param [\"adam\", \"SGD\"] {type:\"raw\"}\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "if OPTIMIZER == 'adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "elif  OPTIMIZER == 'SGD':\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n"
      ],
      "metadata": {
        "id": "KKfuFgHpJy0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 建立訓練流程 \n",
        "\n",
        "在建立好模型以及設定好各項參數之後，我們可以開始建立我們的訓練流程。\n",
        "在原生`PyTorch`之中，並不存在類似`TensorFlow`的`model.fit()`功能，所以我們必須自行將資料導入至模型之中，並取的模型的輸出，最後自行處理向前/向後傳播的流程。這邊簡單的將訓練流程給條列化。\n",
        "\n",
        "1. 取得資料\n",
        "2. 利用`optimizer.zero_grad()`將優化器既存的梯度值清空\n",
        "3. 將資料導入模型，並取得模型輸出\n",
        "4. 利用損失函數計算loss，並利用`loss.backward()`進行反向傳播\n",
        "5. 將新的梯度值利用`optimizer.step()`將參數進行更新\n",
        "\n",
        "因為原生`PyTorch`並未提供計算準確率的功能，也並未提供類似`TensorFlow`中，`model.compile()`中的`metrics`參數直接指定想看到的參數。所以我們需要直接提取模型輸出以及資料標籤進行準確度的評估。\n"
      ],
      "metadata": {
        "id": "wxzEiBKG6O5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    # 定義變數用以儲存單次訓練週期中的loss以及acc值\n",
        "    running_loss = list()\n",
        "    running_acc = 0.0\n",
        "\n",
        "    for times, data in enumerate(trainLoader):\n",
        "\n",
        "        # 宣告模型訓練狀態\n",
        "        model.train()\n",
        "\n",
        "        # 取得資料，並將其傳遞至相應裝置\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # 將影像維度改為第一維度長度為inputs.shape[0]，第二維度為所剩維度展平的長度\n",
        "        inputs = inputs.view(inputs.shape[0], -1)\n",
        "\n",
        "        # 將既存梯度歸零\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 將資料導入模型，並取得模型輸出\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # 將模型輸出轉為整數\n",
        "        predicted = torch.max(outputs.data, 1)[1]\n",
        "        \n",
        "        # 利用損失函數計算loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 進行反向傳播\n",
        "        loss.backward()\n",
        "\n",
        "        # 更新參數\n",
        "        optimizer.step()\n",
        "\n",
        "        # 紀錄週期內的損失值\n",
        "        running_loss.append(loss.item())\n",
        "\n",
        "        # 計算週期內正確預測的數量\n",
        "        running_acc += (labels==predicted).sum().item()\n",
        "    \n",
        "    # 計算週期為單位的loss以及acc\n",
        "    _epoch_loss = torch.tensor(running_loss).mean()\n",
        "    _epoch_acc = running_acc/(len(trainLoader)*64)\n",
        "\n",
        "    # 輸出資訊\n",
        "    print(f\"Epoch : {epoch+1}, Epoch loss: {_epoch_loss:.4f}, Epoch Acc: {_epoch_acc:.2f}\")\n",
        "print('Training Finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKa8K4jmK56R",
        "outputId": "f15faa03-1a15-485b-df05-0f21ada8acfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1, Epoch loss: 0.3909, Epoch Acc: 0.88\n",
            "Epoch : 2, Epoch loss: 0.2502, Epoch Acc: 0.93\n",
            "Epoch : 3, Epoch loss: 0.2253, Epoch Acc: 0.93\n",
            "Epoch : 4, Epoch loss: 0.2185, Epoch Acc: 0.94\n",
            "Epoch : 5, Epoch loss: 0.2102, Epoch Acc: 0.94\n",
            "Epoch : 6, Epoch loss: 0.2062, Epoch Acc: 0.94\n",
            "Epoch : 7, Epoch loss: 0.1880, Epoch Acc: 0.95\n",
            "Epoch : 8, Epoch loss: 0.1860, Epoch Acc: 0.95\n",
            "Epoch : 9, Epoch loss: 0.1932, Epoch Acc: 0.95\n",
            "Epoch : 10, Epoch loss: 0.1791, Epoch Acc: 0.95\n",
            "Training Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 對模型進行驗證"
      ],
      "metadata": {
        "id": "iZB4vbf8TJ7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8g0uRFqsQm2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 建立自定義模型\n",
        "\n",
        "在先前的部份，我們利用`torch.nn.Sequential`建立了順序式模型。對於機器學習，常態下我們需要建立各式各樣的自訂模型結構。在`PyTorch`中，我們可以建立自定義類別物件，繼承`torch.nn.Module`類別，並在此類別內建立我的自訂模型架構。\n",
        "\n"
      ],
      "metadata": {
        "id": "svV-tsqcILch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class my_model(torch.nn.Module):\n",
        "    # 對類別進行初始化\n",
        "    def __init__(self):\n",
        "        # 呼叫繼承對象的__init__函數\n",
        "        super(my_model, self).__init__()\n",
        "        \n",
        "        # 建立模型\n",
        "        self.main = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=784, out_features=128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(in_features=128, out_features=64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(in_features=64, out_features=10),\n",
        "            torch.nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "    \n",
        "    # 定義向前傳播的過程\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "my_model = my_model()\n",
        "my_model"
      ],
      "metadata": {
        "id": "LrK04RqHLVaB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b065d19a-1931-4e5e-815d-975283227371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "my_model(\n",
              "  (main): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
              "    (5): LogSoftmax(dim=1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "除了上方的在類別中利用`Sequential`建立模型的方式，也可以透過以下方式自定義模型。"
      ],
      "metadata": {
        "id": "GhsdCzVtNbhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class my_model_1(torch.nn.Module):\n",
        "    # 對類別進行初始化\n",
        "    def __init__(self):\n",
        "        # 呼叫繼承對象的__init__函數\n",
        "        super(my_model_1, self).__init__()\n",
        "        self.dense_1 = torch.nn.Linear(in_features=784, out_features=128)\n",
        "        self.dense_2 = torch.nn.Linear(in_features=128, out_features=64)\n",
        "        self.dense_3 = torch.nn.Linear(in_features=64, out_features=10)\n",
        "\n",
        "    \n",
        "    # 定義向前傳播的過程\n",
        "    def forward(self, input):\n",
        "        input = self.dense_1(input)\n",
        "        input = F.relu(input)\n",
        "        input = self.dense_2(input)\n",
        "        input = F.relu(input)\n",
        "        input = self.dense_3(input)\n",
        "        return F.log_softmax(input, dim = 1)\n",
        "\n",
        "my_model_1 = my_model_1()\n",
        "my_model_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fECAC0HNlg9",
        "outputId": "cd7ec83c-df3d-43b6-c62d-2d99327d18ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "my_model_1(\n",
              "  (dense_1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (dense_2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (dense_3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 結語\n",
        "\n",
        "在本範例中，我們示範了如何用`nn.Sequential()`函數以繼承`nn.Module`類別的自定義類別來建立一個簡單的機器學習模型。接下來我們將了解如何透過建立`Callback`物件以及利用`Tensorboard`來監控模型的訓練流程。"
      ],
      "metadata": {
        "id": "KF6zEUasD8_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RY1xw2-zOks0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}